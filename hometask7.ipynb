{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9A9Wzsf1RpJ",
        "outputId": "b72ed7ea-5cd3-4ace-ec8c-6889a546fd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents available: 11314\n",
            "Vocabulary size after pruning: 1508\n",
            "Total tokens (after pruning): 120332\n",
            "Initializing topics (K=20)...\n",
            "Start Gibbs sampling: N tokens = 120332\n",
            "Iter 1/80 — iter time 6.3s — total elapsed 6.3s\n",
            "Iter 10/80 — iter time 3.5s — total elapsed 45.0s\n",
            "Iter 20/80 — iter time 2.9s — total elapsed 76.6s\n",
            "Iter 30/80 — iter time 3.2s — total elapsed 108.1s\n",
            "Iter 40/80 — iter time 2.8s — total elapsed 140.1s\n",
            "Iter 50/80 — iter time 3.0s — total elapsed 171.2s\n",
            "Iter 60/80 — iter time 3.0s — total elapsed 203.4s\n",
            "Iter 70/80 — iter time 3.3s — total elapsed 235.5s\n",
            "Iter 80/80 — iter time 2.9s — total elapsed 267.0s\n",
            "Gibbs finished; total time 267.0s\n",
            "\n",
            "TOP-10 words per topic:\n",
            "Topic 1: edu, available, com, file, files, motif, window, version, server, ftp\n",
            "Topic 2: people, israel, jews, israeli, did, men, killed, country, right, like\n",
            "Topic 3: people, gun, law, guns, person, like, state, group, self, rights\n",
            "Topic 4: new, war, time, south, military, secret, power, world, years, plan\n",
            "Topic 5: thanks, edu, need, help, mail, post, know, email, appreciated, send\n",
            "Topic 6: does, question, think, science, know, point, don, time, thinking, having\n",
            "Topic 7: mr, ms, going, think, president, know, don, said, ll, ve\n",
            "Topic 8: think, work, people, ve, administration, president, program, important, make, just\n",
            "Topic 9: 00, dos, good, 50, price, 25, excellent, 20, new, sale\n",
            "Topic 10: space, nasa, earth, com, surface, orbit, data, following, cost, time\n",
            "Topic 11: windows, bit, scsi, mac, card, drive, pc, disk, 32, use\n",
            "Topic 12: 10, 12, la, 16, 11, 15, vs, 20, 30, 17\n",
            "Topic 13: god, believe, church, people, say, does, jesus, religion, christian, true\n",
            "Topic 14: use, 1993, health, 10, number, national, years, april, university, medical\n",
            "Topic 15: car, ve, problem, bike, problems, cars, course, engine, speed, getting\n",
            "Topic 16: like, insurance, make, buy, just, know, use, private, unit, cross\n",
            "Topic 17: said, know, people, don, didn, going, say, got, says, let\n",
            "Topic 18: don, think, like, just, good, really, better, time, know, game\n",
            "Topic 19: max, 45, 14, mr, 34, 75, ma, st, 25, 24\n",
            "Topic 20: key, keys, encryption, use, security, clipper, chip, government, bit, using\n",
            "\n",
            "Topic -> most frequent true label among docs where topic dominates:\n",
            "Topic 1: comp.windows.x (share 0.22) | top words: edu, available, com, file, files, motif, window, version, server, ftp\n",
            "Topic 2: talk.politics.mideast (share 0.39) | top words: people, israel, jews, israeli, did, men, killed, country, right, like\n",
            "Topic 3: talk.politics.guns (share 0.31) | top words: people, gun, law, guns, person, like, state, group, self, rights\n",
            "Topic 4: sci.electronics (share 0.11) | top words: new, war, time, south, military, secret, power, world, years, plan\n",
            "Topic 5: sci.med (share 0.14) | top words: thanks, edu, need, help, mail, post, know, email, appreciated, send\n",
            "Topic 6: sci.med (share 0.13) | top words: does, question, think, science, know, point, don, time, thinking, having\n",
            "Topic 7: talk.politics.misc (share 0.18) | top words: mr, ms, going, think, president, know, don, said, ll, ve\n",
            "Topic 8: talk.politics.misc (share 0.15) | top words: think, work, people, ve, administration, president, program, important, make, just\n",
            "Topic 9: misc.forsale (share 0.48) | top words: 00, dos, good, 50, price, 25, excellent, 20, new, sale\n",
            "Topic 10: sci.space (share 0.45) | top words: space, nasa, earth, com, surface, orbit, data, following, cost, time\n",
            "Topic 11: comp.sys.ibm.pc.hardware (share 0.29) | top words: windows, bit, scsi, mac, card, drive, pc, disk, 32, use\n",
            "Topic 12: rec.sport.hockey (share 0.36) | top words: 10, 12, la, 16, 11, 15, vs, 20, 30, 17\n",
            "Topic 13: soc.religion.christian (share 0.47) | top words: god, believe, church, people, say, does, jesus, religion, christian, true\n",
            "Topic 14: sci.med (share 0.26) | top words: use, 1993, health, 10, number, national, years, april, university, medical\n",
            "Topic 15: rec.autos (share 0.28) | top words: car, ve, problem, bike, problems, cars, course, engine, speed, getting\n",
            "Topic 16: sci.electronics (share 0.15) | top words: like, insurance, make, buy, just, know, use, private, unit, cross\n",
            "Topic 17: rec.autos (share 0.12) | top words: said, know, people, don, didn, going, say, got, says, let\n",
            "Topic 18: rec.sport.baseball (share 0.26) | top words: don, think, like, just, good, really, better, time, know, game\n",
            "Topic 19: comp.os.ms-windows.misc (share 0.25) | top words: max, 45, 14, mr, 34, 75, ma, st, 25, 24\n",
            "Topic 20: sci.crypt (share 0.72) | top words: key, keys, encryption, use, security, clipper, chip, government, bit, using\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import defaultdict, Counter\n",
        "import random, time\n",
        "\n",
        "K = 20\n",
        "sample_size = 2500\n",
        "max_features = 2000\n",
        "min_df = 20\n",
        "max_df = 0.5\n",
        "iterations = 80\n",
        "alpha = 1.0\n",
        "beta = 1.0\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "data = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "docs_all = data.data\n",
        "labels_all = data.target\n",
        "target_names = data.target_names\n",
        "N_docs_total = len(docs_all)\n",
        "print(\"Total documents available:\", N_docs_total)\n",
        "\n",
        "if sample_size is None or sample_size >= N_docs_total:\n",
        "    docs = docs_all\n",
        "    true_labels = labels_all\n",
        "else:\n",
        "    idx = np.random.choice(N_docs_total, sample_size, replace=False)\n",
        "    docs = [docs_all[i] for i in idx]\n",
        "    true_labels = [labels_all[i] for i in idx]\n",
        "\n",
        "D = len(docs)\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=max_features,\n",
        "                             min_df=min_df,\n",
        "                             max_df=max_df)\n",
        "X = vectorizer.fit_transform(docs)\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "V = len(vocab)\n",
        "print(\"Vocabulary size after pruning:\", V)\n",
        "\n",
        "Xcsr = X.tocsr()\n",
        "total_tokens = int(X.sum())\n",
        "print(\"Total tokens (after pruning):\", total_tokens)\n",
        "\n",
        "word_ids = np.empty(total_tokens, dtype=np.int32)\n",
        "doc_ids = np.empty(total_tokens, dtype=np.int32)\n",
        "pos = 0\n",
        "for d in range(D):\n",
        "    row = Xcsr[d]\n",
        "    for w, cnt in zip(row.indices, row.data):\n",
        "        c = int(cnt)\n",
        "        word_ids[pos:pos+c] = w\n",
        "        doc_ids[pos:pos+c] = d\n",
        "        pos += c\n",
        "N = total_tokens\n",
        "\n",
        "print(\"Initializing topics (K={})...\".format(K))\n",
        "z = np.random.randint(0, K, size=N)\n",
        "ndk = np.zeros((D, K), dtype=np.int32)\n",
        "nkw = np.zeros((K, V), dtype=np.int32)\n",
        "nk = np.zeros(K, dtype=np.int32)\n",
        "\n",
        "for i in range(N):\n",
        "    t = z[i]; w = word_ids[i]; d = doc_ids[i]\n",
        "    ndk[d, t] += 1\n",
        "    nkw[t, w] += 1\n",
        "    nk[t] += 1\n",
        "\n",
        "beta_sum = V * beta\n",
        "\n",
        "print(\"Start Gibbs sampling: N tokens =\", N)\n",
        "start = time.time()\n",
        "for it in range(1, iterations+1):\n",
        "    t_iter0 = time.time()\n",
        "    for i in range(N):\n",
        "        w = word_ids[i]; d = doc_ids[i]; t = z[i]\n",
        "        ndk[d, t] -= 1\n",
        "        nkw[t, w] -= 1\n",
        "        nk[t] -= 1\n",
        "\n",
        "        left = ndk[d] + alpha\n",
        "        right = (nkw[:, w] + beta) / (nk + beta_sum)\n",
        "        p = left * right\n",
        "        s = p.sum()\n",
        "        if s <= 0:\n",
        "            p = np.ones(K) / K\n",
        "        else:\n",
        "            p = p / s\n",
        "\n",
        "        r = np.random.rand()\n",
        "        cum = np.cumsum(p)\n",
        "        new_t = int(np.searchsorted(cum, r))\n",
        "        if new_t >= K:\n",
        "            new_t = K-1\n",
        "\n",
        "        z[i] = new_t\n",
        "        ndk[d, new_t] += 1\n",
        "        nkw[new_t, w] += 1\n",
        "        nk[new_t] += 1\n",
        "\n",
        "    if it % 10 == 0 or it == 1:\n",
        "        print(f\"Iter {it}/{iterations} — iter time {time.time()-t_iter0:.1f}s — total elapsed {time.time()-start:.1f}s\")\n",
        "print(\"Gibbs finished; total time {:.1f}s\".format(time.time()-start))\n",
        "\n",
        "phi = (nkw + beta) / (nk[:, None] + beta_sum)\n",
        "theta = (ndk + alpha).astype(float)\n",
        "theta = theta / theta.sum(axis=1, keepdims=True)\n",
        "\n",
        "def top_words(topic, n=10):\n",
        "    ids = np.argsort(phi[topic])[::-1][:n]\n",
        "    return vocab[ids], phi[topic, ids]\n",
        "\n",
        "print(\"\\nTOP-10 words per topic:\")\n",
        "topic_words = []\n",
        "for k in range(K):\n",
        "    words, probs = top_words(k, 10)\n",
        "    topic_words.append(words)\n",
        "    print(f\"Topic {k+1}: {', '.join(words)}\")\n",
        "\n",
        "dominant_topic = np.argmax(theta, axis=1)\n",
        "topic_to_docs = defaultdict(list)\n",
        "for d, tk in enumerate(dominant_topic):\n",
        "    topic_to_docs[tk].append(d)\n",
        "\n",
        "print(\"\\nTopic -> most frequent true label among docs where topic dominates:\")\n",
        "for k in range(K):\n",
        "    docs_k = topic_to_docs.get(k, [])\n",
        "    if not docs_k:\n",
        "        print(f\"Topic {k+1}: (no dominant docs)\")\n",
        "        continue\n",
        "    labs = [true_labels[d] for d in docs_k]\n",
        "    most_common_label, cnt = Counter(labs).most_common(1)[0]\n",
        "    frac = cnt / len(labs)\n",
        "    print(f\"Topic {k+1}: {target_names[most_common_label]} (share {frac:.2f}) | top words: {', '.join(topic_words[k])}\")"
      ]
    }
  ]
}